# enhanced_interplm/configs/default_config.yaml
# Default configuration for Enhanced InterPLM training

# Data configuration
data:
  root_dir: ${oc.env:ENHANCED_INTERPLM_DATA,data/processed}
  batch_size: 16
  num_workers: 4
  max_seq_length: 512
  include_structures: true
  include_evolution: true
  include_annotations: true
  cache_embeddings: true
  
# Model architecture
model:
  # ESM base model
  esm_model: esm2_t6_8M_UR50D
  esm_layers: [1, 2, 3, 4, 5, 6]
  
  # Temporal SAE
  temporal_sae:
    input_dim: 320  # ESM-2 8M hidden size
    hidden_dim: 512
    dict_size: 2560  # 8x expansion
    num_attention_heads: 8
    dropout: 0.1
    time_decay_factor: 0.9
    
  # Biophysics constraints
  biophysics:
    enabled: true
    physics_weight: 0.1
    constraint_types:
      - hydrophobicity
      - charge
      - size
      - hbond
      - spatial
      
  # Hierarchical mapping
  hierarchical:
    aa_property_dim: 64
    ss_hidden_dim: 128
    domain_hidden_dim: 256
    num_domain_types: 100
    
  # Circuit discovery
  circuits:
    mutual_info_threshold: 0.1
    causal_threshold: 0.3
    min_motif_size: 3
    max_motif_size: 8
    validation_threshold: 0.7

# Training configuration
training:
  num_epochs: 100
  learning_rate: 1e-3
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip: 1.0
  
  # Loss weights
  losses:
    reconstruction: 1.0
    sparsity: 0.1
    physics: 0.1
    circuit_coherence: 0.05
    hierarchical_consistency: 0.05
    
  # Regularization
  l1_penalty: 0.1
  l1_annealing_pct: 0.1
  
  # Optimization
  optimizer: AdamW
  scheduler: cosine_annealing
  
  # Checkpointing
  save_every: 10
  eval_every: 5
  early_stopping_patience: 20
  keep_best_k: 3

# Evaluation configuration
evaluation:
  metrics:
    - reconstruction_mse
    - variance_explained
    - feature_sparsity
    - circuit_coherence
    - biological_relevance
  
  # Circuit analysis
  circuit_analysis_freq: 100
  max_circuits_to_analyze: 50
  
  # Biological evaluation
  biological:
    eval_secondary_structure: true
    eval_domains: true
    eval_go_terms: true
    eval_conservation: true
    motif_library: default

# Experiment tracking
tracking:
  use_wandb: true
  wandb_project: enhanced-interplm
  wandb_entity: ${oc.env:WANDB_ENTITY,null}
  
  use_tensorboard: true
  tensorboard_dir: ${tracking.log_dir}/tensorboard
  
  log_dir: logs/${experiment.name}
  checkpoint_dir: checkpoints/${experiment.name}
  results_dir: results/${experiment.name}

# Experiment settings
experiment:
  name: ${now:%Y%m%d_%H%M%S}_enhanced_interplm
  seed: 42
  deterministic: true
  mixed_precision: true
  compile_model: false  # PyTorch 2.0 compile

# Hardware configuration
hardware:
  device: cuda
  gpu_id: 0
  distributed: false
  num_gpus: 1
  
# Visualization
visualization:
  save_plots: true
  plot_format: png
  dpi: 300
  style: dark
  
  # Specific visualizations
  plot_feature_evolution: true
  plot_circuits: true
  plot_hierarchical_mapping: true
  plot_3d_constraints: true
  
  # Animation settings
  create_animations: false
  animation_fps: 10

---
# enhanced_interplm/configs/small_model_config.yaml
# Configuration for smaller ESM model (35M parameters)

defaults:
  - default_config

model:
  esm_model: esm2_t12_35M_UR50D
  esm_layers: [1, 3, 6, 9, 12]
  
  temporal_sae:
    input_dim: 480  # ESM-2 35M hidden size
    hidden_dim: 768
    dict_size: 3840  # 8x expansion

training:
  batch_size: 32  # Can use larger batch with smaller model
  learning_rate: 2e-3

---
# enhanced_interplm/configs/large_model_config.yaml  
# Configuration for large ESM model (650M parameters)

defaults:
  - default_config

model:
  esm_model: esm2_t33_650M_UR50D
  esm_layers: [1, 6, 12, 18, 24, 30, 33]
  
  temporal_sae:
    input_dim: 1280  # ESM-2 650M hidden size
    hidden_dim: 2048
    dict_size: 10240  # 8x expansion
    
training:
  batch_size: 4  # Smaller batch for memory
  gradient_accumulation_steps: 4
  learning_rate: 5e-4
  mixed_precision: true

hardware:
  distributed: true
  num_gpus: 4

---
# enhanced_interplm/configs/ablation_config.yaml
# Configuration for ablation studies

defaults:
  - default_config

# Ablation settings
ablation:
  # Disable specific components
  disable_temporal_attention: false
  disable_biophysics: false
  disable_circuits: false
  disable_hierarchical: false
  
  # Modify architectures
  use_standard_sae: false  # Use standard SAE instead of temporal
  use_random_init: false  # Random initialization instead of pretrained
  
  # Data ablations
  shuffle_sequences: false
  mask_structures: false
  remove_evolution: false

model:
  temporal_sae:
    dict_size: ${eval:${model.temporal_sae.input_dim} * 4}  # Smaller expansion
    
training:
  num_epochs: 50  # Shorter training for ablations

---
# enhanced_interplm/configs/inference_config.yaml
# Configuration for inference only

defaults:
  - default_config

mode: inference

inference:
  checkpoint_path: ${oc.env:CHECKPOINT_PATH}
  batch_size: 64
  save_features: true
  save_circuits: true
  save_predictions: true
  
  # Feature extraction settings
  features:
    layers: all
    aggregate: mean  # mean, max, or concat
    normalize: true
    
  # Circuit analysis
  circuits:
    top_k: 100
    min_importance: 0.5
    
  # Output settings
  output:
    format: hdf5  # hdf5, npz, or parquet
    compression: gzip
    save_attention: false